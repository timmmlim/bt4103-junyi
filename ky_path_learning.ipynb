{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "log_problem = pd.read_csv('Log_Problem.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to this article: https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community #This part of networkx, for community detection, needs to be imported separately.\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = log_problem['ucid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_TW</th>\n",
       "      <th>uuid</th>\n",
       "      <th>ucid</th>\n",
       "      <th>upid</th>\n",
       "      <th>problem_number</th>\n",
       "      <th>exercise_problem_repeat_session</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>total_sec_taken</th>\n",
       "      <th>total_attempt_cnt</th>\n",
       "      <th>used_hint_cnt</th>\n",
       "      <th>is_hint_used</th>\n",
       "      <th>is_downgrade</th>\n",
       "      <th>is_upgrade</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-26 21:00:00 UTC</td>\n",
       "      <td>FLy+lviglNR5Y1l0Xiijnl6QHySBcpKHJLCtQ6ogm2Q=</td>\n",
       "      <td>KDOmuTrY/IJzDP4kIgIYCBiGyTymsJ8Iy4cDB35WGYg=</td>\n",
       "      <td>Vbs92l4JmdiWkUEm/iahxnUTaac2oN1IlUtXB7JcfoE=</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-17 16:30:00 UTC</td>\n",
       "      <td>+Gqj2nalc6M9fusyVECTC0AN7UQdDQTXESIuElkDltU=</td>\n",
       "      <td>COZ39Wo+uIUO2s7c2VGEHjJf6Vx0xifxVAiaeHtaTdk=</td>\n",
       "      <td>Ek+pIeHNNoEo0tGEq91eBcBmGgy3+A5RWhpj95zTyHM=</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-15 19:15:00 UTC</td>\n",
       "      <td>6D5QN8j8ng/VR74ES3A0zqAj0bIFFyaKjKEj8ZyXjQ8=</td>\n",
       "      <td>TwyqyV1uJYlDAX8wX/PtTCVZEBo/APIVfTzzleGkNCQ=</td>\n",
       "      <td>1MBa2f5Qog4JBoAuUfJf0fxeJctdEirAqKgfsg246eI=</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-05 14:45:00 UTC</td>\n",
       "      <td>GgTZuCqZXObthtK6GAwqvlHrTMm5pKHWeezQxL/pcKc=</td>\n",
       "      <td>tBo6ECyT8IlKAM8UhQHWkqv92PRLcSiwuerfC7vNX+w=</td>\n",
       "      <td>kdMy2nG+QVMjPkuaMEWs0yV/sYZVoG1vm7zM0fCy+qk=</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-14 16:45:00 UTC</td>\n",
       "      <td>JMNKWoU0CkMSzgQ8bCnmCYlD8jEzAVge3lHMYLXKM2g=</td>\n",
       "      <td>vVpSKAMQbTMvtdERR0ksOeRmmaFt0R210t4Z//0RpPA=</td>\n",
       "      <td>jjPR8fmkLSFoCQQYB4g6kI8mgdcK3sKtMirKUvfmZIk=</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp_TW                                          uuid  \\\n",
       "0  2019-05-26 21:00:00 UTC  FLy+lviglNR5Y1l0Xiijnl6QHySBcpKHJLCtQ6ogm2Q=   \n",
       "1  2019-05-17 16:30:00 UTC  +Gqj2nalc6M9fusyVECTC0AN7UQdDQTXESIuElkDltU=   \n",
       "2  2019-05-15 19:15:00 UTC  6D5QN8j8ng/VR74ES3A0zqAj0bIFFyaKjKEj8ZyXjQ8=   \n",
       "3  2019-05-05 14:45:00 UTC  GgTZuCqZXObthtK6GAwqvlHrTMm5pKHWeezQxL/pcKc=   \n",
       "4  2019-05-14 16:45:00 UTC  JMNKWoU0CkMSzgQ8bCnmCYlD8jEzAVge3lHMYLXKM2g=   \n",
       "\n",
       "                                           ucid  \\\n",
       "0  KDOmuTrY/IJzDP4kIgIYCBiGyTymsJ8Iy4cDB35WGYg=   \n",
       "1  COZ39Wo+uIUO2s7c2VGEHjJf6Vx0xifxVAiaeHtaTdk=   \n",
       "2  TwyqyV1uJYlDAX8wX/PtTCVZEBo/APIVfTzzleGkNCQ=   \n",
       "3  tBo6ECyT8IlKAM8UhQHWkqv92PRLcSiwuerfC7vNX+w=   \n",
       "4  vVpSKAMQbTMvtdERR0ksOeRmmaFt0R210t4Z//0RpPA=   \n",
       "\n",
       "                                           upid  problem_number  \\\n",
       "0  Vbs92l4JmdiWkUEm/iahxnUTaac2oN1IlUtXB7JcfoE=              18   \n",
       "1  Ek+pIeHNNoEo0tGEq91eBcBmGgy3+A5RWhpj95zTyHM=               4   \n",
       "2  1MBa2f5Qog4JBoAuUfJf0fxeJctdEirAqKgfsg246eI=               9   \n",
       "3  kdMy2nG+QVMjPkuaMEWs0yV/sYZVoG1vm7zM0fCy+qk=               2   \n",
       "4  jjPR8fmkLSFoCQQYB4g6kI8mgdcK3sKtMirKUvfmZIk=               6   \n",
       "\n",
       "   exercise_problem_repeat_session  is_correct  total_sec_taken  \\\n",
       "0                                2        True               33   \n",
       "1                                1        True                8   \n",
       "2                                1        True               17   \n",
       "3                                1        True               10   \n",
       "4                                1        True               98   \n",
       "\n",
       "   total_attempt_cnt  used_hint_cnt  is_hint_used is_downgrade is_upgrade  \\\n",
       "0                  1              0         False        False       True   \n",
       "1                  1              0         False          NaN        NaN   \n",
       "2                  1              0         False          NaN        NaN   \n",
       "3                  1              0         False          NaN        NaN   \n",
       "4                  1              0         False          NaN        NaN   \n",
       "\n",
       "   level  \n",
       "0      3  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_problem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_problem.sort_values(by=['timestamp_TW'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_content = log_problem.groupby(['uuid'], sort=False)['ucid'].apply(lambda x: x.tolist()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_content['to'] = user_to_content['ucid'].apply(lambda x: x[1:])\n",
    "user_to_content['from'] = user_to_content['ucid'].apply(lambda x: x[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the content mapping of a particular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_TW</th>\n",
       "      <th>uuid</th>\n",
       "      <th>ucid</th>\n",
       "      <th>upid</th>\n",
       "      <th>problem_number</th>\n",
       "      <th>exercise_problem_repeat_session</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>total_sec_taken</th>\n",
       "      <th>total_attempt_cnt</th>\n",
       "      <th>used_hint_cnt</th>\n",
       "      <th>is_hint_used</th>\n",
       "      <th>is_downgrade</th>\n",
       "      <th>is_upgrade</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105021</th>\n",
       "      <td>2018-08-01 07:45:00 UTC</td>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=</td>\n",
       "      <td>9Ksy3bdm0HTi0D+cdKSBKDQJjo5GNahep91FqHUrpts=</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11669798</th>\n",
       "      <td>2018-08-01 07:45:00 UTC</td>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=</td>\n",
       "      <td>/Wgjdl2BsldHZDdXXvzwGimusaMX548lqV2b7PgwXAs=</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768239</th>\n",
       "      <td>2018-08-01 07:45:00 UTC</td>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=</td>\n",
       "      <td>vsGlLPd9C58B8myBoGMGre2pDHjh62eRjsqX57D98fU=</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10163558</th>\n",
       "      <td>2018-08-01 07:45:00 UTC</td>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>hv7kHCAIdj7thZUmlqz553leG5bFNYgzXmLfB5m4Xvw=</td>\n",
       "      <td>h3CI/U4QJd6mjYE5xRH8QEst8lRG7otYIz+q1V6Och4=</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372688</th>\n",
       "      <td>2018-08-01 07:45:00 UTC</td>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=</td>\n",
       "      <td>OSuDd4rDo2muXXwwJRU2DQVHkk6/JOGgNzfzNi4PMJM=</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp_TW  \\\n",
       "105021    2018-08-01 07:45:00 UTC   \n",
       "11669798  2018-08-01 07:45:00 UTC   \n",
       "3768239   2018-08-01 07:45:00 UTC   \n",
       "10163558  2018-08-01 07:45:00 UTC   \n",
       "8372688   2018-08-01 07:45:00 UTC   \n",
       "\n",
       "                                                  uuid  \\\n",
       "105021    U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "11669798  U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "3768239   U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "10163558  U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "8372688   U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "\n",
       "                                                  ucid  \\\n",
       "105021    CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=   \n",
       "11669798  CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=   \n",
       "3768239   CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=   \n",
       "10163558  hv7kHCAIdj7thZUmlqz553leG5bFNYgzXmLfB5m4Xvw=   \n",
       "8372688   CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=   \n",
       "\n",
       "                                                  upid  problem_number  \\\n",
       "105021    9Ksy3bdm0HTi0D+cdKSBKDQJjo5GNahep91FqHUrpts=               2   \n",
       "11669798  /Wgjdl2BsldHZDdXXvzwGimusaMX548lqV2b7PgwXAs=               1   \n",
       "3768239   vsGlLPd9C58B8myBoGMGre2pDHjh62eRjsqX57D98fU=               5   \n",
       "10163558  h3CI/U4QJd6mjYE5xRH8QEst8lRG7otYIz+q1V6Och4=               5   \n",
       "8372688   OSuDd4rDo2muXXwwJRU2DQVHkk6/JOGgNzfzNi4PMJM=               3   \n",
       "\n",
       "          exercise_problem_repeat_session  is_correct  total_sec_taken  \\\n",
       "105021                                  1        True                8   \n",
       "11669798                                1        True               10   \n",
       "3768239                                 1        True                6   \n",
       "10163558                                1        True                5   \n",
       "8372688                                 1        True                4   \n",
       "\n",
       "          total_attempt_cnt  used_hint_cnt  is_hint_used is_downgrade  \\\n",
       "105021                    1              0         False          NaN   \n",
       "11669798                  1              0         False          NaN   \n",
       "3768239                   1              0         False        False   \n",
       "10163558                  1              0         False        False   \n",
       "8372688                   1              0         False          NaN   \n",
       "\n",
       "         is_upgrade  level  \n",
       "105021          NaN      0  \n",
       "11669798        NaN      0  \n",
       "3768239        True      1  \n",
       "10163558       True      1  \n",
       "8372688         NaN      0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_problem[log_problem['uuid'] == 'U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4='].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that a user can do multiple questions consecutively(and hence multiple instances of the same 'ucid' is updated consecutively in the log). Thus i just drop rows when the course content is the same consecutively. \n",
    "- This is done in when creating data for networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of content path with no consecutive same content\n",
    "user_to_content['unique_ucid'] = user_to_content['ucid'].apply(lambda x: [content for index, content in enumerate(x[:-1]) if x[index]!=x[index+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that for this user there is no repeated content\n",
    "tc1_uuid = 'U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4='\n",
    "tc_d = user_to_content[user_to_content['uuid']==tc1_uuid]\n",
    "assert len(tc_d['unique_ucid']) == len(np.unique(tc_d['uuid']))\n",
    "\n",
    "# user that only did one content will have empty list of contents\n",
    "tc2_uuid = 'Oj3eQxD25uE+DeBkWLSlwUQSYyFms0cBF6zGvMDpDIo=' # row 72753\n",
    "tc_d = user_to_content[user_to_content['uuid']==tc2_uuid]\n",
    "assert len(np.unique(tc_d['ucid'].values.tolist()[0]))==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop those rows with empty list\n",
    "user_to_content['to_keep'] = user_to_content['unique_ucid'].apply(lambda x: np.nan if len(x)==0 else 1)\n",
    "user_to_content = user_to_content.dropna(subset=['to_keep'])\n",
    "\n",
    "# Create new source node list and the corresponding target list without any repeated consecutive contents\n",
    "user_to_content.loc[:,'to2'] = user_to_content['unique_ucid'].apply(lambda x: x[1:])\n",
    "user_to_content.loc[:,'from2'] = user_to_content['unique_ucid'].apply(lambda x: x[:-1])\n",
    "user_to_content['to2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Define the edge weights for the exercise?\n",
    "    - Do we have the information to compute the weihgt such that we can compare paths against one another\n",
    "    for src in range(num_exercise):\n",
    "        for tgt in range(num_problems):\n",
    "        - (Frequency of student for src to tgt) * (accuracy of student obtained by student from src to tgt)\n",
    "        \n",
    "> Define the edge weights for the problem?\n",
    "    - Do length of the problem sequence\n",
    "    for i in range(num_problems):\n",
    "        - (Frequency of student for problem i) * (accuracy of student for problem i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(fname):\n",
    "    G = nx.DiGraph()\n",
    "    d = simplejson.load(open(fname))\n",
    "    for item in d:\n",
    "        for attribute, value in item.iteritems():\n",
    "            subject_id, object_id = value['subject_id'], value['object_id']\n",
    "            if G.has_edge(subject_id, object_id):\n",
    "                # we added this one before, just increase the weight by one\n",
    "                G[subject_id][object_id]['weight'] += 1 is_correct(src) -> is_correct(tgt)\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(subject_id, object_id, weight=1)\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directed Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for index, row in user_to_content.iterrows():\n",
    "    user = row['uuid']\n",
    "    from_list = row['from2']\n",
    "    to_list = row['to2']\n",
    "    # always check that the number \n",
    "    assert len(from_list) == len(to_list)\n",
    "    for src, tgt in zip(from_list, to_list):\n",
    "        if G.has_edge(src, tgt):\n",
    "            G[src][tgt]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(src, tgt, weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unDirected Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for index, row in user_to_content.sample(10000).iterrows():\n",
    "    user = row['uuid']\n",
    "    from_list = row['from2']\n",
    "    to_list = row['to2']\n",
    "    # always check that the number \n",
    "    assert len(from_list) == len(to_list)\n",
    "    for src, tgt in zip(from_list, to_list):\n",
    "        if G.has_edge(src, tgt):\n",
    "            G[src][tgt]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(src, tgt, weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of time, i sampled 10000 lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ucid</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>unique_ucid</th>\n",
       "      <th>to_keep</th>\n",
       "      <th>to2</th>\n",
       "      <th>from2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=</td>\n",
       "      <td>[CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...</td>\n",
       "      <td>[CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...</td>\n",
       "      <td>[CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...</td>\n",
       "      <td>[CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[hv7kHCAIdj7thZUmlqz553leG5bFNYgzXmLfB5m4Xvw=,...</td>\n",
       "      <td>[CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZtYcfQTg++4WtnOkIdgI7O4odEyGxOP4ZchUy1ENNIY=</td>\n",
       "      <td>[KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...</td>\n",
       "      <td>[KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...</td>\n",
       "      <td>[KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...</td>\n",
       "      <td>[KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[suDCkQs0H6kriJ78abNYmrtyOrBj80FflKWAiW3yfD8=,...</td>\n",
       "      <td>[KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>j/Prqubd0XhHmteIdpCvOcq7IgV4Ib9qLXF1gwNI5ww=</td>\n",
       "      <td>[h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...</td>\n",
       "      <td>[h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...</td>\n",
       "      <td>[h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...</td>\n",
       "      <td>[h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...</td>\n",
       "      <td>[h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tj/HawoHmjMZJFe3o2B5zeASGbnAuNUtzNd1YrtcHdw=</td>\n",
       "      <td>[TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...</td>\n",
       "      <td>[TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...</td>\n",
       "      <td>[TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...</td>\n",
       "      <td>[TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[NoskBq4Br1/gsHaS0Pkuhdx67DlKfRiCcj2jMAWpRS8=,...</td>\n",
       "      <td>[TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHtLCC16j7YXj6gCouJXMcLe7UapiMEO1DBn5xUfM6Q=</td>\n",
       "      <td>[PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...</td>\n",
       "      <td>[4p305X/NqL1Zx9j6BDJEpiQx090kFF3o/LTTGJt5s/E=,...</td>\n",
       "      <td>[PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...</td>\n",
       "      <td>[PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[4p305X/NqL1Zx9j6BDJEpiQx090kFF3o/LTTGJt5s/E=,...</td>\n",
       "      <td>[PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72749</th>\n",
       "      <td>PiqckXsI/J2wiFgc6UkrSkHi0ZVx/+oHrTaFExnc0us=</td>\n",
       "      <td>[jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...</td>\n",
       "      <td>[jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...</td>\n",
       "      <td>[jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...</td>\n",
       "      <td>[jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[aQys6A/5FjGwd7dQ0KQ4h/5zmPrM7lbkwj2ZkuhMepo=,...</td>\n",
       "      <td>[jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72751</th>\n",
       "      <td>yzzVBDVLH6D6WYl+lcybce8c0KIObCsssioFEXn2EzQ=</td>\n",
       "      <td>[cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...</td>\n",
       "      <td>[5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...</td>\n",
       "      <td>[cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...</td>\n",
       "      <td>[cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...</td>\n",
       "      <td>[cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72752</th>\n",
       "      <td>saB6CvIM32G0+RDry4wCo5ro6A1kI3fKYaKsjwQc+T4=</td>\n",
       "      <td>[P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...</td>\n",
       "      <td>[P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...</td>\n",
       "      <td>[P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...</td>\n",
       "      <td>[P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[wZCkmlfJ0G2xyiucQ85Ms+DMGXRSg8yhQ+MiwtSHS30=]</td>\n",
       "      <td>[P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72755</th>\n",
       "      <td>90OCKSK8bkx4M+KdLllF8XNzykbJldJtxWaxQRuxPWY=</td>\n",
       "      <td>[XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...</td>\n",
       "      <td>[OnztoLe/KRAGCzT1fhys2SSvoBd4VIDTcFpP5fMpU68=,...</td>\n",
       "      <td>[XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...</td>\n",
       "      <td>[XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OnztoLe/KRAGCzT1fhys2SSvoBd4VIDTcFpP5fMpU68=,...</td>\n",
       "      <td>[XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72756</th>\n",
       "      <td>0Zo0lVHE8Y33fjFhq9XiO/C1ha5rwl9sQ5Hd8h5dEjw=</td>\n",
       "      <td>[dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...</td>\n",
       "      <td>[dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...</td>\n",
       "      <td>[dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...</td>\n",
       "      <td>[dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[vhz7ZImgcFLciakjMFdmn0S/He85wqQkhLiYXRPpu18=,...</td>\n",
       "      <td>[dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62915 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               uuid  \\\n",
       "0      U+lqK/FKWkPuoNUM1AbGyrKZfXeQrRRoKOToKrjqDt4=   \n",
       "1      ZtYcfQTg++4WtnOkIdgI7O4odEyGxOP4ZchUy1ENNIY=   \n",
       "2      j/Prqubd0XhHmteIdpCvOcq7IgV4Ib9qLXF1gwNI5ww=   \n",
       "3      tj/HawoHmjMZJFe3o2B5zeASGbnAuNUtzNd1YrtcHdw=   \n",
       "4      AHtLCC16j7YXj6gCouJXMcLe7UapiMEO1DBn5xUfM6Q=   \n",
       "...                                             ...   \n",
       "72749  PiqckXsI/J2wiFgc6UkrSkHi0ZVx/+oHrTaFExnc0us=   \n",
       "72751  yzzVBDVLH6D6WYl+lcybce8c0KIObCsssioFEXn2EzQ=   \n",
       "72752  saB6CvIM32G0+RDry4wCo5ro6A1kI3fKYaKsjwQc+T4=   \n",
       "72755  90OCKSK8bkx4M+KdLllF8XNzykbJldJtxWaxQRuxPWY=   \n",
       "72756  0Zo0lVHE8Y33fjFhq9XiO/C1ha5rwl9sQ5Hd8h5dEjw=   \n",
       "\n",
       "                                                    ucid  \\\n",
       "0      [CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...   \n",
       "1      [KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...   \n",
       "2      [h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...   \n",
       "3      [TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...   \n",
       "4      [PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...   \n",
       "...                                                  ...   \n",
       "72749  [jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...   \n",
       "72751  [cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...   \n",
       "72752  [P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...   \n",
       "72755  [XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...   \n",
       "72756  [dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...   \n",
       "\n",
       "                                                      to  \\\n",
       "0      [CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...   \n",
       "1      [KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...   \n",
       "2      [h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...   \n",
       "3      [TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...   \n",
       "4      [4p305X/NqL1Zx9j6BDJEpiQx090kFF3o/LTTGJt5s/E=,...   \n",
       "...                                                  ...   \n",
       "72749  [jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...   \n",
       "72751  [5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...   \n",
       "72752  [P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...   \n",
       "72755  [OnztoLe/KRAGCzT1fhys2SSvoBd4VIDTcFpP5fMpU68=,...   \n",
       "72756  [dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...   \n",
       "\n",
       "                                                    from  \\\n",
       "0      [CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...   \n",
       "1      [KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...   \n",
       "2      [h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...   \n",
       "3      [TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...   \n",
       "4      [PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...   \n",
       "...                                                  ...   \n",
       "72749  [jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...   \n",
       "72751  [cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...   \n",
       "72752  [P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...   \n",
       "72755  [XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...   \n",
       "72756  [dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...   \n",
       "\n",
       "                                             unique_ucid  to_keep  \\\n",
       "0      [CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...      1.0   \n",
       "1      [KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...      1.0   \n",
       "2      [h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...      1.0   \n",
       "3      [TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...      1.0   \n",
       "4      [PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...      1.0   \n",
       "...                                                  ...      ...   \n",
       "72749  [jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...      1.0   \n",
       "72751  [cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...      1.0   \n",
       "72752  [P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=,...      1.0   \n",
       "72755  [XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...      1.0   \n",
       "72756  [dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...      1.0   \n",
       "\n",
       "                                                     to2  \\\n",
       "0      [hv7kHCAIdj7thZUmlqz553leG5bFNYgzXmLfB5m4Xvw=,...   \n",
       "1      [suDCkQs0H6kriJ78abNYmrtyOrBj80FflKWAiW3yfD8=,...   \n",
       "2      [5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...   \n",
       "3      [NoskBq4Br1/gsHaS0Pkuhdx67DlKfRiCcj2jMAWpRS8=,...   \n",
       "4      [4p305X/NqL1Zx9j6BDJEpiQx090kFF3o/LTTGJt5s/E=,...   \n",
       "...                                                  ...   \n",
       "72749  [aQys6A/5FjGwd7dQ0KQ4h/5zmPrM7lbkwj2ZkuhMepo=,...   \n",
       "72751  [5gAwpmADo34wxzAw/auRsRgPsj+RvWPNWhOj3SdP0Ss=,...   \n",
       "72752     [wZCkmlfJ0G2xyiucQ85Ms+DMGXRSg8yhQ+MiwtSHS30=]   \n",
       "72755  [OnztoLe/KRAGCzT1fhys2SSvoBd4VIDTcFpP5fMpU68=,...   \n",
       "72756  [vhz7ZImgcFLciakjMFdmn0S/He85wqQkhLiYXRPpu18=,...   \n",
       "\n",
       "                                                   from2  \n",
       "0      [CPI+5YCeEmhqdk6znJeii6jJUNl1QWGEvwCUJ6uLflg=,...  \n",
       "1      [KpJfQutqPUoUbwWZ5oNFKeVhvKqXZcqL69vsPEHg9jY=,...  \n",
       "2      [h327f3+M2F6lvbXTOgg35VC4pac6YD68c7hjS2DPPiA=,...  \n",
       "3      [TKDAjftSYyI2PNmAkXdInD39P5BcPNa6a+qx0fxm4RA=,...  \n",
       "4      [PGbCr6wCmzlyPoKCBQv8aOM0NW8HVk4PW+IU4iR6GRI=,...  \n",
       "...                                                  ...  \n",
       "72749  [jvSzlPLRjqqstMszjfw2Y1+0IOmVTo8P8l/rpJzPgNU=,...  \n",
       "72751  [cvnGl+iUiKphfjbPcE0G7/HRE/Q51L3w0x0WhVLy1cs=,...  \n",
       "72752     [P9MBH2y9lTr+ueuFDr6VDrx3GSS1/MvPLRyklqdSyVM=]  \n",
       "72755  [XPjtEuV2qgexNe8re/JecZlBLnzABLhXY2kd8BFHlRg=,...  \n",
       "72756  [dVKfHzCsi1dOBhd343bsUNF4dowsiANnvRBkWqToitg=,...  \n",
       "\n",
       "[62915 rows x 8 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_to_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 0.1152747659295939\n"
     ]
    }
   ],
   "source": [
    "# DENSITY\n",
    "# Density is the ratio of the actual edges in the network to all possible edges in the network\n",
    "# Interpretation: 0 to 1, the bigger the more dense your network. 1 is a perfectly connect network\n",
    "# This mean that \n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSSP\n",
    "######  SSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "src =list(G.nodes)[0]\n",
    "tgt =list(G.nodes)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ew: 1.accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path: ['svTmVFY7kPYUWAdDc4l5Zaim90TY7RGkFLs/eaU4FVc=', 'DnZ+4F0wfZyawol4t+/bAvIm5PTPYUC/NSZakKr8g4I=', 'bGhEHzDDpgeN/wftiCCViepN+b6lTXEnxj1/8DxWeZ4=', 'RLVg2oNL54gAjL5WK5vK9JDSj1xUds0tDwVqAAfDTxg=']\n",
      "Length of that path: 3\n"
     ]
    }
   ],
   "source": [
    "# SSSP\n",
    "# Trim the edge weights first before we try to run SSSP?\n",
    "# Given some domain knowledge, eg what course the student wants to take, run the SSSP algorithm\n",
    "# Some thing to consider is that if an intermediate node has a high degree centrality(more on that later),\n",
    "# The SSSP may take very long since several shortest paths run through him as the mediator\n",
    "# (This provide us with some hint that DSTA should focus on nodes with the highest degree centrality, \n",
    "# I.E make the courses better since these are important courses to take!)\n",
    "path_length = nx.shortest_path(G, source=src, target=tgt)\n",
    "\n",
    "print(f\"Shortest path:\", path_length)\n",
    "print(\"Length of that path:\", len(path_length)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components and their Diameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Network diameter of largest component: 4\n"
     ]
    }
   ],
   "source": [
    "# COMPONENTS, AND COMPONENTS AND THIER DIAMETERS\n",
    "# LOOK AT THE NUMBER OF COMPONENTS\n",
    "# If we have more than one components, then this means that some course contents are very different from the \n",
    "# Eg interpretation: There is a learning path frokm length 8 between the two furthest apart courses in the network.\n",
    "\n",
    "# If your Graph has more than one component, this will return False:\n",
    "# print(nx.is_strongly_connected(G))\n",
    "print(nx.is_connected(G))\n",
    "\n",
    "\n",
    "# Next, use nx.connected_components to get the list of components,\n",
    "# then use the max() command to find the largest one:\n",
    "components = nx.connected_components(G)\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "# Create a \"subgraph\" of just the largest component\n",
    "# Then calculate the diameter of the subgraph, just like you did with density.\n",
    "# This ensures that nx.diameter will not throw an error\n",
    "subgraph = G.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triadic Closure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSITIVITY\n",
    "# Clustering coefficient is one way to measure Triadic closure as it measures the clustering tendency.\n",
    "# Transitivity is the ratio of all triangles over all possible triangles\n",
    "triadic_closure = nx.transitivity(G)\n",
    "print(\"Triadic closure:\", triadic_closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 1: node A: metric(A) = 2, node b: 2. node c: 8\n",
    "cluster 2: node A: 1, node b: 9, node c: 5\n",
    "cluster 3: node A: 1, node b: 3, node c: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEGREE\n",
    "# Course with the highest degree are those most connected to other courses. These nodes are call Hubs.\n",
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree') # store all the degrees in the node\n",
    "### Find the top few nodes with the highest degree\n",
    "#  Eg print(G.nodes['William Penn'])\n",
    "# sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "# print(\"Top 20 nodes by degree:\")\n",
    "# for d in sorted_degree[:20]:\n",
    "#     print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness and eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EIGENVECTOR CENTRALITY\n",
    "# Eigenvector centrality useful for understanding which nodes can get information to other nodes quickly.\n",
    "# Example: if you know a lot of well-connected people, you can spread a message very efficiently.\n",
    "# So these are courses that are related to many of the important courses, and should be placed into the learning path?\n",
    "\n",
    "\n",
    "## BETWEENNESS CENTRALITY\n",
    "# Betweenness centrality looks at all the shortest paths that pass through a particular node. (Thus takes very long to calculate)\n",
    "# Tells us which nodes are important not because they have lots of connections themselves, but because they stand between groups, \n",
    "# Giving the network connectivity and cohesion\n",
    "\n",
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "\n",
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)\n",
    "    \n",
    "### You can see that nodes that have high degree also have high betweenness centrality\n",
    "# What if you want to know learning path have high betweenness centrality and low degree?\n",
    "# I.e which high betweenness nodes are unexpected?\n",
    "## If we have high betweenness centrality and low degree, these could mean that the nodes are important nodes connecting\n",
    "## distant parts of the graph.\n",
    "\n",
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMUNITIES\n",
    "## Community detection with modularity\n",
    "## Modularity is a measure of relative density in your network. \n",
    "## A community has a high density relative to other nodes within its module, but low density with those outside\n",
    "## Modularity gives you an overall score of how fractious your network is, and that is a score\n",
    "# that can be used to partition the network and return the inidividual communities\n",
    "\n",
    "# Very dense networks are often more difficult to split into sensible partitions.\n",
    "# Luckily, networks are not all that dense. Our network is only 0.223\n",
    "# Although there are some built in approahces like 'minimum cut', modularity is not included in networkX\n",
    "# We just use an addiitonal python module, YAY\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "## the method greedy_modularity_communities() tries to determine the number of communities appropriate for the graphs\n",
    "## Group all nodes into the subsets based on these communities.\n",
    "\n",
    "modularity_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')\n",
    "\n",
    "## Then in each of these communities, you do your centrality measures like EIGENVECTOR etc etc \n",
    "\n",
    "# First get a list of just the nodes in that class\n",
    "class0 = [n for n in G.nodes() if G.nodes[n]['modularity'] == 0]\n",
    "\n",
    "# Then create a dictionary of the eigenvector centralities of those nodes\n",
    "class0_eigenvector = {n:G.nodes[n]['eigenvector'] for n in class0}\n",
    "\n",
    "# Then sort that dictionary and print the first 5 results\n",
    "class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Modularity Class 0 Sorted by Eigenvector Centrality:\")\n",
    "for node in class0_sorted_by_eigenvector[:5]:\n",
    "    print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])\n",
    "    \n",
    "    \n",
    "### Using eigenvector centrality as a ranking can give you a sense of the important people within this modularity class\n",
    "##  In smaller networks, it may be a common task to find a list of all the modularity classes and their members\n",
    "\n",
    "for i,c in enumerate(communities): # Loop through the list of communities\n",
    "    if len(c) > 2: # Filter out modularity classes with 2 or fewer nodes\n",
    "        print('Class '+str(i)+':', list(c)) # Print out the classes and their members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can ignore this part below, it is just the compiled version of the entire thing from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 0.2237843991007143\n"
     ]
    }
   ],
   "source": [
    "## For each clusterm, perform a network analysis to determine if the learning path of the courses are indeed differnt\n",
    "# \"Quantitative metrics let you differentiate networks, learn about their topologies, and turn a jumble of nodes and edges into something you can learn from.\"\n",
    "\n",
    "# Find the shortest path from one content to the other?\n",
    "\n",
    "# DENSITY\n",
    "# Density is the ratio of the actual edges in the network to all possible edges in the network\n",
    "# Interpretation: 0 to 1, the bigger the more dense your network. 1 is a perfectly connect network\n",
    "# This mean that \n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)\n",
    "\n",
    "# SSSP\n",
    "# Trim the edge weights first before we try to run SSSP?\n",
    "# Given some domain knowledge, eg what course the student wants to take, run the SSSP algorithm\n",
    "# Some thing to consider is that if an intermediate node has a high degree centrality(more on that later),\n",
    "# The SSSP may take very long since several shortest paths run through him as the mediator\n",
    "# (This provide us with some hint that DSTA should focus on nodes with the highest degree centrality, \n",
    "# I.E make the courses better since these are important courses to take!)\n",
    "fell_whitehead_path = nx.shortest_path(G, source=\"Margaret Fell\", target=\"George Whitehead\")\n",
    "\n",
    "print(\"Shortest path between Fell and Whitehead:\", fell_whitehead_path)\n",
    "print(\"Length of that path:\", len(fell_whitehead_path)-1)\n",
    "\n",
    "### DIAMETER\n",
    "# We can also look at dimaeter, which the longest of the shortest path.\n",
    "# This gives as a sense of the network's overall size. \n",
    "# I.e this means that the longest it will take for a student to accomplish this is ____. (provided we used time as edge weights) \n",
    "# Should we tell our client to look into the longest short path, and tell them that these are the learning paths that are\n",
    "# taking the longest to complete?\n",
    "\n",
    "## If the graph is not connected, then the nx.diameter will throw error\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)\n",
    "\n",
    "# COMPONENTS, AND COMPONENTS AND THIER DIAMETERS\n",
    "# LOOK AT THE NUMBER OF COMPONENTS\n",
    "# If we have more than one components, then this means that some course contents are very different from the \n",
    "# Eg interpretation: There is a learning path frokm length 8 between the two furthest apart courses in the network.\n",
    "\n",
    "# If your Graph has more than one component, this will return False:\n",
    "print(nx.is_connected(G))\n",
    "\n",
    "# Next, use nx.connected_components to get the list of components,\n",
    "# then use the max() command to find the largest one:\n",
    "components = nx.connected_components(G)\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "# Create a \"subgraph\" of just the largest component\n",
    "# Then calculate the diameter of the subgraph, just like you did with density.\n",
    "# This ensures that nx.diameter will not throw an error\n",
    "subgraph = G.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)\n",
    "\n",
    "\n",
    "# Triadic closure: if two people know the same person, they are likdely to know each other\n",
    "# The number of these enclosed triangles in the network can be used to\n",
    "# find clusters and communities of courses that are related to each other?\n",
    "\n",
    "# TRANSITIVITY\n",
    "# Clustering coefficient is one way to measure Triadic closure as it measures the clustering tendency.\n",
    "# Transitivity is the ratio of all triangles over all possible triangles\n",
    "triadic_closure = nx.transitivity(G)\n",
    "print(\"Triadic closure:\", triadic_closure)\n",
    "\n",
    "\n",
    "# CENTRALITY\n",
    "# Answers the questions of which nodes are the most important?\n",
    "# Degree, betweenness centrality, eigenvector centrality etc etc\n",
    "\n",
    "## DEGREE\n",
    "# Course with the highest degree are those most connected to other courses. These nodes are call Hubs.\n",
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree') # store all the degrees in the node\n",
    "### Find the top few nodes with the highest degree\n",
    "#  Eg print(G.nodes['William Penn'])\n",
    "# sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "# print(\"Top 20 nodes by degree:\")\n",
    "# for d in sorted_degree[:20]:\n",
    "#     print(d)\n",
    "\n",
    "## EIGENVECTOR CENTRALITY\n",
    "# Eigenvector centrality useful for understanding which nodes can get information to other nodes quickly.\n",
    "# Example: if you know a lot of well-connected people, you can spread a message very efficiently.\n",
    "# So these are courses that are related to many of the important courses, and should be placed into the learning path?\n",
    "\n",
    "\n",
    "## BETWEENNESS CENTRALITY\n",
    "# Betweenness centrality looks at all the shortest paths that pass through a particular node. (Thus takes very long to calculate)\n",
    "# Tells us which nodes are important not because they have lots of connections themselves, but because they stand between groups, \n",
    "# Giving the network connectivity and cohesion\n",
    "\n",
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "\n",
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)\n",
    "    \n",
    "### You can see that nodes that have high degree also have high betweenness centrality\n",
    "# What if you want to know learning path have high betweenness centrality and low degree?\n",
    "# I.e which high betweenness nodes are unexpected?\n",
    "## If we have high betweenness centrality and low degree, these could mean that the nodes are important nodes connecting\n",
    "## distant parts of the graph.\n",
    "\n",
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)\n",
    "    \n",
    "    \n",
    "# COMMUNITIES\n",
    "## Community detection with modularity\n",
    "## Modularity is a measure of relative density in your network. \n",
    "## A community has a high density relative to other nodes within its module, but low density with those outside\n",
    "## Modularity gives you an overall score of how fractious your network is, and that is a score\n",
    "# that can be used to partition the network and return the inidividual communities\n",
    "\n",
    "# Very dense networks are often more difficult to split into sensible partitions.\n",
    "# Luckily, networks are not all that dense. Our network is only 0.223\n",
    "# Although there are some built in approahces like 'minimum cut', modularity is not included in networkX\n",
    "# We just use an addiitonal python module, YAY\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "## the method greedy_modularity_communities() tries to determine the number of communities appropriate for the graphs\n",
    "## Group all nodes into the subsets based on these communities.\n",
    "\n",
    "modularity_dict = {} # Create a blank dictionary\n",
    "for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community\n",
    "    for name in c: # Loop through each person in a community\n",
    "        modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to.\n",
    "\n",
    "# Now you can add modularity information like we did the other metrics\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')\n",
    "\n",
    "## Then in each of these communities, you do your centrality measures like EIGENVECTOR etc etc \n",
    "\n",
    "# First get a list of just the nodes in that class\n",
    "class0 = [n for n in G.nodes() if G.nodes[n]['modularity'] == 0]\n",
    "\n",
    "# Then create a dictionary of the eigenvector centralities of those nodes\n",
    "class0_eigenvector = {n:G.nodes[n]['eigenvector'] for n in class0}\n",
    "\n",
    "# Then sort that dictionary and print the first 5 results\n",
    "class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Modularity Class 0 Sorted by Eigenvector Centrality:\")\n",
    "for node in class0_sorted_by_eigenvector[:5]:\n",
    "    print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])\n",
    "    \n",
    "    \n",
    "### Using eigenvector centrality as a ranking can give you a sense of the important people within this modularity class\n",
    "##  In smaller networks, it may be a common task to find a list of all the modularity classes and their members\n",
    "\n",
    "for i,c in enumerate(communities): # Loop through the list of communities\n",
    "    if len(c) > 2: # Filter out modularity classes with 2 or fewer nodes\n",
    "        print('Class '+str(i)+':', list(c)) # Print out the classes and their members\n",
    "        \n",
    "# Notice that we are filtering out any modularity class with two or fewer nodes.\n",
    "\n",
    "# itâ€™s usually a good idea to get the global modularity score first \n",
    "# to determine whether youâ€™ll learn anything by partitioning your network according to modularity. \n",
    "# To see the overall modularity score, \n",
    "# take the communities you calculated \n",
    "# with communities = community.best_partition(G) and run global_modularity = community.modularity(communities, G). \n",
    "# Then just print(global_modularity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
